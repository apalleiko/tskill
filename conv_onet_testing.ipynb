{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from policy.dataset.ms2CONet import ManiSkillDataset, ManiSkillConvONetDataset_V2\n",
    "from policy import config\n",
    "import numpy as np\n",
    "import trimesh\n",
    "import torch\n",
    "\n",
    "cfg = config.load_config(\"assets/perception/conv_onet_test.yaml\")\n",
    "\n",
    "dataset = ManiSkillDataset(cfg[\"data\"][\"dataset\"])\n",
    "cam2world = dataset[0][\"obs\"][\"camera_param\"][\"base_camera\"][\"cam2world_gl\"][0]\n",
    "groundtruth = ManiSkillDataset(cfg[\"data\"][\"ground_truth\"])\n",
    "inputs = ManiSkillConvONetDataset_V2(cfg[\"data\"][\"dataset\"], cfg[\"data\"][\"ground_truth\"], [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sapien_pose_to_opencv_extrinsic(sapien_pose_matrix: np.ndarray) -> np.ndarray:\n",
    "    sapien2opencv = np.array(\n",
    "        [\n",
    "            [0.0, -1.0, 0.0, 0.0],\n",
    "            [0.0, 0.0, -1.0, 0.0],\n",
    "            [1.0, 0.0, 0.0, 0.0],\n",
    "            [0.0, 0.0, 0.0, 1.0],\n",
    "        ],\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "    ex = sapien2opencv @ np.linalg.inv(sapien_pose_matrix) # world -> camera\n",
    "\n",
    "    return ex\n",
    "\n",
    "cam_ex = dataset[0][\"obs\"][\"camera_param\"][\"base_camera\"][\"extrinsic_cv\"][0]\n",
    "\n",
    "def invert_transform(H: np.ndarray):\n",
    "    assert H.shape[-2:] == (4, 4), H.shape\n",
    "    H_inv = H.copy()\n",
    "    R_T = np.swapaxes(H[..., :3, :3], -1, -2)\n",
    "    H_inv[..., :3, :3] = R_T\n",
    "    H_inv[..., :3, 3:] = -R_T @ H[..., :3, 3:]\n",
    "    return H_inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam0 = 16384\n",
    "cam1 = 16384*2\n",
    "cam2 = 16384*3\n",
    "cam3 = 16384*4\n",
    " \n",
    "# print(cam2world)\n",
    "points = torch.asarray(groundtruth[0][\"obs\"][\"pointcloud\"][\"xyzw\"][0, 0:cam0, :])\n",
    "# cam_ex = sapien_pose_to_opencv_extrinsic(cam_ex)\n",
    "inv_cam_ex = invert_transform((np.asarray(cam_ex)))\n",
    "\n",
    "points = points @ torch.tensor(inv_cam_ex)\n",
    "colors = groundtruth[0][\"obs\"][\"pointcloud\"][\"rgb\"][0, 0:cam0, :]\n",
    "trans_pcd = trimesh.points.PointCloud(points[..., 0:3], colors=colors)\n",
    "\n",
    "data = inputs.datamap[(0, 0, 0)][\"input_data\"]\n",
    "input_points = data[:, 0:3]\n",
    "input_colors = data[:, 3:] * 255.0\n",
    "ones = torch.ones((16384, 1))\n",
    "\n",
    "input_points = torch.cat((input_points, ones), dim=1).float()\n",
    "input_colors = torch.cat((input_colors, ones), dim=1).int()\n",
    "# input_points = input_points @ torch.tensor(inv_c2w)\n",
    "\n",
    "input_pcd = trimesh.points.PointCloud(input_points[..., 0:3], input_colors)\n",
    "\n",
    "scene = trimesh.Scene([input_pcd, trans_pcd])\n",
    "scene.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from policy.dataset.ms2CONet import get_MS_loaders\n",
    "from policy import config\n",
    "\n",
    "cfg = config.load_config(\"assets/perception/conv_onet_test.yaml\")\n",
    "# dataset = ManiSkillConvONetDataset_V2(cfg[\"data\"][\"dataset\"], indices=[0])\n",
    "model = config.get_model(cfg)\n",
    "# print(dataset)\n",
    "train_loader, val_loader = get_MS_loaders(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "data = train_loader.dataset[-10]\n",
    "cam2world = dataset[0][\"obs\"][\"camera_param\"][\"base_camera\"][\"cam2world_gl\"][0]\n",
    "\n",
    "# input visualization \n",
    "input_points = data[\"input_data\"][:, 0:3]\n",
    "print(input_points.shape)\n",
    "input_colors = data[\"input_data\"][:, 3:] * 255.0\n",
    "ones = torch.ones((16384, 1))\n",
    "input_colors = torch.cat((input_colors, ones), dim=1).int()\n",
    "\n",
    "xyzw = torch.cat([input_points, ones != 0], dim=-1).reshape(\n",
    "                input_points.shape[0], -1, 4) @ cam2world.transpose(1, 2)\n",
    "\n",
    "\n",
    "# rot_mat1 = torch.tensor([[np.cos(np.pi/2), -np.sin(np.pi/2), 0],\n",
    "#                         [np.sin(np.pi/2), np.cos(np.pi/2), 0],\n",
    "#                         [0, 0, 1]]).float()\n",
    "\n",
    "# rot_mat2 = torch.tensor([\n",
    "#                         [np.cos(np.pi),0, -np.sin(np.pi)],\n",
    "#                         [0, 1, 0],\n",
    "#                         [np.sin(np.pi), 0, np.cos(np.pi)]\n",
    "#                         ]).float()\n",
    "\n",
    "# refl_mat = torch.tensor([[-1, 0, 0],\n",
    "#                          [0, 1, 0],\n",
    "#                          [0, 0, -1]]).float()\n",
    "\n",
    "# input_points = torch.mm(input_points, refl_mat)\n",
    "# input_points = torch.mm(input_points, rot_mat1)\n",
    "# input_points = torch.mm(input_points, rot_mat2)\n",
    "\n",
    "\n",
    "input_pcd = trimesh.points.PointCloud(input_points, input_colors)\n",
    "input_scene = input_pcd.scene()\n",
    "\n",
    "# gt visualization\n",
    "gt_points = data[\"gt_points\"]\n",
    "print(gt_points.shape)\n",
    "gt_colors = data[\"gt_colors\"]\n",
    "gt_colors = torch.cat((gt_colors, ones), dim=1).int()\n",
    "\n",
    "gt_pcd = trimesh.points.PointCloud(gt_points, gt_colors)\n",
    "gt_scene = gt_pcd.scene()\n",
    "\n",
    "scene = trimesh.Scene([input_pcd])\n",
    "scene.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Indices: [1, 0]\n",
      "Validation Indices: [2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152it [00:11, 13.58it/s]\n",
      "137it [00:10, 13.48it/s]\n",
      "172it [00:12, 13.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from policy.dataset.ms2CONet import ManiSkillDataset, ManiSkillConvONetDataset_V3, get_MS_loaders_V3\n",
    "from policy import config\n",
    "from policy.perception.conv_onet.training import Trainer\n",
    "import numpy as np\n",
    "import trimesh\n",
    "import torch\n",
    "\n",
    "cfg = config.load_config(\"assets/perception/conv_onet_test.yaml\")\n",
    "# data = ManiSkillConvONetDataset_V3(cfg[\"data\"][\"dataset\"], [0])\n",
    "train_loader, val_loader = get_MS_loaders_V3(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = data[0][\"inputs\"].unsqueeze(0)\n",
    "# points = data[0][\"points\"].unsqueeze(0)\n",
    "# print(points.shape)\n",
    "# print(inputs.shape)\n",
    "\n",
    "model = config.get_model(cfg)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "trainer = Trainer(model, optimizer, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103679.265625"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next(iter(train_loader))\n",
    "#  trainer.compute_loss(data[0])\n",
    "trainer.train_step(next(iter(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/UROP/Pele/tskill/policy/perception/conv_onet/training.py:71\u001b[0m, in \u001b[0;36mTrainer.eval_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     68\u001b[0m inputs \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m'\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(points\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     69\u001b[0m voxels_occ \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoxels\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m points_iou \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpoints_iou\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[1;32m     72\u001b[0m occ_iou \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoints_iou.occ\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     74\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m points\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "trainer.eval_step(val_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
