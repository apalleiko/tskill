{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrl/miniconda3/envs/tskill/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating new train & val indices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting all training data info:: 100%|██████████| 50/50 [00:03<00:00, 13.43it/s]\n",
      "/home/mrl/miniconda3/envs/tskill/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mrl/miniconda3/envs/tskill/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recomputing scaling functions...\n",
      "Computing seperate gripper scaling\n",
      "Computing normal quantile transform\n",
      "Computing linear scaling\n",
      "Adding batch dimension to returned data!\n",
      "50 0\n",
      "freezing state encoder network!\n",
      "/home/mrl/Documents/Projects/tskill/out/VAE/009/model_best.pt\n",
      "=> Loading checkpoint from local file...\n",
      "load state dict: <All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TSkillCVAE(\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (encoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (input_embed): Embedding(4, 1024)\n",
       "  (stt_encoder): ResnetStateEncoder(\n",
       "    (backbone): Joiner(\n",
       "      (0): Backbone(\n",
       "        (body): IntermediateLayerGetter(\n",
       "          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (layer1): Sequential(\n",
       "            (0): BasicBlock(\n",
       "              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "            )\n",
       "            (1): BasicBlock(\n",
       "              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (layer2): Sequential(\n",
       "            (0): BasicBlock(\n",
       "              (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): FrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): BasicBlock(\n",
       "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (layer3): Sequential(\n",
       "            (0): BasicBlock(\n",
       "              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): FrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): BasicBlock(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (layer4): Sequential(\n",
       "            (0): BasicBlock(\n",
       "              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): FrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): BasicBlock(\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): PositionEmbeddingSine()\n",
       "    )\n",
       "  )\n",
       "  (image_proj): Linear(in_features=512, out_features=1024, bias=True)\n",
       "  (image_feat_norm): LayerNorm((16, 1024), eps=1e-05, elementwise_affine=True)\n",
       "  (enc_action_proj): Linear(in_features=7, out_features=1024, bias=True)\n",
       "  (enc_action_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (enc_state_proj): Linear(in_features=9, out_features=1024, bias=True)\n",
       "  (enc_state_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (enc_image_proj): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (enc_src_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (enc_tgt_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (enc_z): Linear(in_features=1024, out_features=32, bias=True)\n",
       "  (dec_z): Linear(in_features=16, out_features=1024, bias=True)\n",
       "  (dec_input_state_proj): Linear(in_features=9, out_features=1024, bias=True)\n",
       "  (dec_input_state_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (dec_input_z_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (dec_src_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (dec_tgt_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (dec_action_joint_proj): Linear(in_features=1024, out_features=6, bias=True)\n",
       "  (dec_action_gripper_proj): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=1, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from policy import config\n",
    "from policy.dataset.dataset_loaders import dataset_loader\n",
    "from policy.checkpoints import CheckpointIO\n",
    "\n",
    "model_dir = \"/home/mrl/Documents/Projects/tskill/out/VAE/009\"\n",
    "cfg_path = os.path.join(model_dir, \"config.yaml\")\n",
    "cfg = config.load_config(cfg_path, None)\n",
    "\n",
    "# Dataset\n",
    "cfg[\"data\"][\"pad\"] = False\n",
    "cfg[\"data\"][\"augment\"] = False\n",
    "cfg[\"data\"][\"full_seq\"] = False\n",
    "cfg[\"data\"][\"max_count\"] = 100\n",
    "cfg[\"data\"][\"val_split\"] = 0\n",
    "\n",
    "# Load only the full episode version of the dataset\n",
    "train_dataset, val_dataset = dataset_loader(cfg, return_datasets=True, \n",
    "                                            save_override=True,\n",
    "                                            preshuffle=False,\n",
    "                                            fullseq_override=True,\n",
    "                                            )\n",
    "print(len(train_dataset), len(val_dataset))\n",
    "# Model\n",
    "model = config.get_model(cfg, device=\"cuda\")\n",
    "checkpoint_io = CheckpointIO(model_dir, model=model)\n",
    "load_dict = checkpoint_io.load(\"model_best.pt\")\n",
    "stt_encoder = model.stt_encoder\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################### 0\n",
      "==>> dataset_file: /home/mrl/Documents/Projects/tskill/LIBERO/libero/datasets/libero_90/KITCHEN_SCENE1_open_the_top_drawer_of_the_cabinet_and_put_the_bowl_in_it_demo.hdf5\n",
      "==>> eps: 0\n",
      "==>> trajectory: <HDF5 group \"/data/demo_0\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 189, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (189, 2, 16, 512)\n",
      "==>> img_pe.shape: (189, 2, 16, 1024)\n",
      "==>> eps: 1\n",
      "==>> trajectory: <HDF5 group \"/data/demo_1\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 184, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (184, 2, 16, 512)\n",
      "==>> img_pe.shape: (184, 2, 16, 1024)\n",
      "==>> eps: 2\n",
      "==>> trajectory: <HDF5 group \"/data/demo_2\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 196, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (196, 2, 16, 512)\n",
      "==>> img_pe.shape: (196, 2, 16, 1024)\n",
      "==>> eps: 3\n",
      "==>> trajectory: <HDF5 group \"/data/demo_3\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 184, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (184, 2, 16, 512)\n",
      "==>> img_pe.shape: (184, 2, 16, 1024)\n",
      "==>> eps: 4\n",
      "==>> trajectory: <HDF5 group \"/data/demo_4\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 175, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (175, 2, 16, 512)\n",
      "==>> img_pe.shape: (175, 2, 16, 1024)\n",
      "==>> eps: 5\n",
      "==>> trajectory: <HDF5 group \"/data/demo_5\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 195, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (195, 2, 16, 512)\n",
      "==>> img_pe.shape: (195, 2, 16, 1024)\n",
      "==>> eps: 6\n",
      "==>> trajectory: <HDF5 group \"/data/demo_6\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 187, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (187, 2, 16, 512)\n",
      "==>> img_pe.shape: (187, 2, 16, 1024)\n",
      "==>> eps: 7\n",
      "==>> trajectory: <HDF5 group \"/data/demo_7\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 188, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (188, 2, 16, 512)\n",
      "==>> img_pe.shape: (188, 2, 16, 1024)\n",
      "==>> eps: 8\n",
      "==>> trajectory: <HDF5 group \"/data/demo_8\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 195, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (195, 2, 16, 512)\n",
      "==>> img_pe.shape: (195, 2, 16, 1024)\n",
      "==>> eps: 9\n",
      "==>> trajectory: <HDF5 group \"/data/demo_9\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 188, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (188, 2, 16, 512)\n",
      "==>> img_pe.shape: (188, 2, 16, 1024)\n",
      "==>> eps: 10\n",
      "==>> trajectory: <HDF5 group \"/data/demo_10\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 219, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (219, 2, 16, 512)\n",
      "==>> img_pe.shape: (219, 2, 16, 1024)\n",
      "==>> eps: 11\n",
      "==>> trajectory: <HDF5 group \"/data/demo_11\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 211, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (211, 2, 16, 512)\n",
      "==>> img_pe.shape: (211, 2, 16, 1024)\n",
      "==>> eps: 12\n",
      "==>> trajectory: <HDF5 group \"/data/demo_12\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 204, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (204, 2, 16, 512)\n",
      "==>> img_pe.shape: (204, 2, 16, 1024)\n",
      "==>> eps: 13\n",
      "==>> trajectory: <HDF5 group \"/data/demo_13\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 212, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (212, 2, 16, 512)\n",
      "==>> img_pe.shape: (212, 2, 16, 1024)\n",
      "==>> eps: 14\n",
      "==>> trajectory: <HDF5 group \"/data/demo_14\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 233, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (233, 2, 16, 512)\n",
      "==>> img_pe.shape: (233, 2, 16, 1024)\n",
      "==>> eps: 15\n",
      "==>> trajectory: <HDF5 group \"/data/demo_15\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 206, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (206, 2, 16, 512)\n",
      "==>> img_pe.shape: (206, 2, 16, 1024)\n",
      "==>> eps: 16\n",
      "==>> trajectory: <HDF5 group \"/data/demo_16\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 214, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (214, 2, 16, 512)\n",
      "==>> img_pe.shape: (214, 2, 16, 1024)\n",
      "==>> eps: 17\n",
      "==>> trajectory: <HDF5 group \"/data/demo_17\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 181, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (181, 2, 16, 512)\n",
      "==>> img_pe.shape: (181, 2, 16, 1024)\n",
      "==>> eps: 18\n",
      "==>> trajectory: <HDF5 group \"/data/demo_18\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 218, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (218, 2, 16, 512)\n",
      "==>> img_pe.shape: (218, 2, 16, 1024)\n",
      "==>> eps: 19\n",
      "==>> trajectory: <HDF5 group \"/data/demo_19\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 252, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (252, 2, 16, 512)\n",
      "==>> img_pe.shape: (252, 2, 16, 1024)\n",
      "==>> eps: 20\n",
      "==>> trajectory: <HDF5 group \"/data/demo_20\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 189, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (189, 2, 16, 512)\n",
      "==>> img_pe.shape: (189, 2, 16, 1024)\n",
      "==>> eps: 21\n",
      "==>> trajectory: <HDF5 group \"/data/demo_21\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 237, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (237, 2, 16, 512)\n",
      "==>> img_pe.shape: (237, 2, 16, 1024)\n",
      "==>> eps: 22\n",
      "==>> trajectory: <HDF5 group \"/data/demo_22\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 222, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (222, 2, 16, 512)\n",
      "==>> img_pe.shape: (222, 2, 16, 1024)\n",
      "==>> eps: 23\n",
      "==>> trajectory: <HDF5 group \"/data/demo_23\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 198, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (198, 2, 16, 512)\n",
      "==>> img_pe.shape: (198, 2, 16, 1024)\n",
      "==>> eps: 24\n",
      "==>> trajectory: <HDF5 group \"/data/demo_24\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 197, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (197, 2, 16, 512)\n",
      "==>> img_pe.shape: (197, 2, 16, 1024)\n",
      "==>> eps: 25\n",
      "==>> trajectory: <HDF5 group \"/data/demo_25\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 185, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (185, 2, 16, 512)\n",
      "==>> img_pe.shape: (185, 2, 16, 1024)\n",
      "==>> eps: 26\n",
      "==>> trajectory: <HDF5 group \"/data/demo_26\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 206, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (206, 2, 16, 512)\n",
      "==>> img_pe.shape: (206, 2, 16, 1024)\n",
      "==>> eps: 27\n",
      "==>> trajectory: <HDF5 group \"/data/demo_27\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 196, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (196, 2, 16, 512)\n",
      "==>> img_pe.shape: (196, 2, 16, 1024)\n",
      "==>> eps: 28\n",
      "==>> trajectory: <HDF5 group \"/data/demo_28\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 189, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (189, 2, 16, 512)\n",
      "==>> img_pe.shape: (189, 2, 16, 1024)\n",
      "==>> eps: 29\n",
      "==>> trajectory: <HDF5 group \"/data/demo_29\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 168, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (168, 2, 16, 512)\n",
      "==>> img_pe.shape: (168, 2, 16, 1024)\n",
      "==>> eps: 30\n",
      "==>> trajectory: <HDF5 group \"/data/demo_30\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 179, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (179, 2, 16, 512)\n",
      "==>> img_pe.shape: (179, 2, 16, 1024)\n",
      "==>> eps: 31\n",
      "==>> trajectory: <HDF5 group \"/data/demo_31\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 195, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (195, 2, 16, 512)\n",
      "==>> img_pe.shape: (195, 2, 16, 1024)\n",
      "==>> eps: 32\n",
      "==>> trajectory: <HDF5 group \"/data/demo_32\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 221, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (221, 2, 16, 512)\n",
      "==>> img_pe.shape: (221, 2, 16, 1024)\n",
      "==>> eps: 33\n",
      "==>> trajectory: <HDF5 group \"/data/demo_33\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 182, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (182, 2, 16, 512)\n",
      "==>> img_pe.shape: (182, 2, 16, 1024)\n",
      "==>> eps: 34\n",
      "==>> trajectory: <HDF5 group \"/data/demo_34\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 196, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (196, 2, 16, 512)\n",
      "==>> img_pe.shape: (196, 2, 16, 1024)\n",
      "==>> eps: 35\n",
      "==>> trajectory: <HDF5 group \"/data/demo_35\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 199, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (199, 2, 16, 512)\n",
      "==>> img_pe.shape: (199, 2, 16, 1024)\n",
      "==>> eps: 36\n",
      "==>> trajectory: <HDF5 group \"/data/demo_36\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 192, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (192, 2, 16, 512)\n",
      "==>> img_pe.shape: (192, 2, 16, 1024)\n",
      "==>> eps: 37\n",
      "==>> trajectory: <HDF5 group \"/data/demo_37\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 249, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (249, 2, 16, 512)\n",
      "==>> img_pe.shape: (249, 2, 16, 1024)\n",
      "==>> eps: 38\n",
      "==>> trajectory: <HDF5 group \"/data/demo_38\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 202, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (202, 2, 16, 512)\n",
      "==>> img_pe.shape: (202, 2, 16, 1024)\n",
      "==>> eps: 39\n",
      "==>> trajectory: <HDF5 group \"/data/demo_39\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 185, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (185, 2, 16, 512)\n",
      "==>> img_pe.shape: (185, 2, 16, 1024)\n",
      "==>> eps: 40\n",
      "==>> trajectory: <HDF5 group \"/data/demo_40\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 213, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (213, 2, 16, 512)\n",
      "==>> img_pe.shape: (213, 2, 16, 1024)\n",
      "==>> eps: 41\n",
      "==>> trajectory: <HDF5 group \"/data/demo_41\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 192, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (192, 2, 16, 512)\n",
      "==>> img_pe.shape: (192, 2, 16, 1024)\n",
      "==>> eps: 42\n",
      "==>> trajectory: <HDF5 group \"/data/demo_42\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 197, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (197, 2, 16, 512)\n",
      "==>> img_pe.shape: (197, 2, 16, 1024)\n",
      "==>> eps: 43\n",
      "==>> trajectory: <HDF5 group \"/data/demo_43\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 215, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (215, 2, 16, 512)\n",
      "==>> img_pe.shape: (215, 2, 16, 1024)\n",
      "==>> eps: 44\n",
      "==>> trajectory: <HDF5 group \"/data/demo_44\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 215, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (215, 2, 16, 512)\n",
      "==>> img_pe.shape: (215, 2, 16, 1024)\n",
      "==>> eps: 45\n",
      "==>> trajectory: <HDF5 group \"/data/demo_45\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 194, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (194, 2, 16, 512)\n",
      "==>> img_pe.shape: (194, 2, 16, 1024)\n",
      "==>> eps: 46\n",
      "==>> trajectory: <HDF5 group \"/data/demo_46\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 191, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (191, 2, 16, 512)\n",
      "==>> img_pe.shape: (191, 2, 16, 1024)\n",
      "==>> eps: 47\n",
      "==>> trajectory: <HDF5 group \"/data/demo_47\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 189, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (189, 2, 16, 512)\n",
      "==>> img_pe.shape: (189, 2, 16, 1024)\n",
      "==>> eps: 48\n",
      "==>> trajectory: <HDF5 group \"/data/demo_48\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 195, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (195, 2, 16, 512)\n",
      "==>> img_pe.shape: (195, 2, 16, 1024)\n",
      "==>> eps: 49\n",
      "==>> trajectory: <HDF5 group \"/data/demo_49\" (6 members)>\n",
      "==>> trajectory: <KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']>\n",
      "MISSING ITEMS!!!!\n",
      "==>> rgb.shape: torch.Size([1, 191, 2, 3, 128, 128])\n",
      "==>> img_feat.shape: (191, 2, 16, 512)\n",
      "==>> img_pe.shape: (191, 2, 16, 1024)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# for dataset in val_dataset.sequence_datasets:\n",
    "for dataset in [val_dataset]:\n",
    "    val_dataset.data.close()\n",
    "\n",
    "j = 0\n",
    "# for dataset in train_dataset.sequence_datasets:\n",
    "for dataset in [train_dataset]:\n",
    "    print(f\"##################### {j}\")\n",
    "    dataset_path = dataset.dataset_file\n",
    "    print(f\"==>> dataset_file: {dataset_path}\")\n",
    "    dataset.data.close()\n",
    "    dataset_file = h5py.File(dataset_path, \"r\")\n",
    "    dataset_file.close()\n",
    "    dataset_file = h5py.File(dataset_path, \"r+\")\n",
    "    dataset.data = dataset_file\n",
    "    dataset.episodes = dataset_file[\"data\"]\n",
    "    for i in range(len(dataset)):\n",
    "        eps = dataset.owned_indices[i]\n",
    "        print(f\"==>> eps: {eps}\")\n",
    "        trajectory = dataset.episodes[f\"demo_{eps}\"]\n",
    "        print(f\"==>> trajectory: {trajectory}\")\n",
    "        print(f\"==>> trajectory: {trajectory.keys()}\")\n",
    "        if \"resnet18\" in trajectory[\"obs\"].keys():\n",
    "            if \"img_feat\" in trajectory[\"obs\"][\"resnet18\"].keys():\n",
    "                add_feats = False\n",
    "            else:\n",
    "                add_feats = True\n",
    "            if \"img_pe_1024\" in trajectory[\"obs\"][\"resnet18\"].keys():\n",
    "                add_pe = False\n",
    "            else:\n",
    "                add_pe = True\n",
    "\n",
    "            if not add_feats and not add_pe:\n",
    "                continue\n",
    "            else:\n",
    "                print(\"MISSING ITEMS!!!!\")\n",
    "        \n",
    "        rgb = dataset[i][\"rgb\"].to(model._device)\n",
    "        print(f\"==>> rgb.shape: {rgb.shape}\")\n",
    "        with torch.no_grad():\n",
    "            img_feat, img_pe = stt_encoder(rgb)\n",
    "        img_feat, img_pe = img_feat[:,0,...].detach().cpu().numpy(), img_pe[:,0,...].detach().cpu().numpy()\n",
    "        print(f\"==>> img_feat.shape: {img_feat.shape}\")\n",
    "        print(f\"==>> img_pe.shape: {img_pe.shape}\")\n",
    "        # raise ValueError\n",
    "        # # del trajectory[\"obs\"][\"image\"]\n",
    "        if add_feats:\n",
    "            trajectory.create_dataset(\"obs/resnet18/img_feat\",\n",
    "                                    data=img_feat,\n",
    "                                    dtype=img_feat.dtype,\n",
    "                                    compression=\"gzip\",\n",
    "                                    compression_opts=5,)\n",
    "        if add_pe:\n",
    "            trajectory.create_dataset(\"obs/resnet18/img_pe_1024\",\n",
    "                                    data=img_pe,\n",
    "                                    dtype=img_pe.dtype,\n",
    "                                    compression=\"gzip\",\n",
    "                                    compression_opts=5,)\n",
    "        \n",
    "    j += 1\n",
    "\n",
    "    dataset_file.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tskill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
