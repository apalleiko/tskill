{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing data info file\n",
      "Loading indices from file: out/PegInsertion/Plan/003/data_info.pickle\n",
      "Overriding full seq config!\n",
      "Loading action and state scaling from file\n",
      "Adding batch dimension to returned data!\n",
      "freezing state encoder network!\n",
      "/home/mrl/Documents/Projects/tskill/out/PegInsertion/VAE/001/model_best.pt\n",
      "=> Loading checkpoint from local file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrl/miniconda3/envs/tskill/lib/python3.11/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/home/mrl/miniconda3/envs/tskill/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mrl/miniconda3/envs/tskill/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/mrl/Documents/Projects/tskill/policy/checkpoints.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(filename, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load state dict: <All keys matched successfully>\n",
      "Freezing state encoder network!\n",
      "/home/mrl/Documents/Projects/tskill/out/PegInsertion/Plan/003/model_best.pt\n",
      "=> Loading checkpoint from local file...\n",
      "load state dict: <All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TSkillPlan(\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "          (dropout3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (vae): TSkillCVAE(\n",
       "    (decoder): Transformer(\n",
       "      (encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-3): 4 x TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.2, inplace=False)\n",
       "            (dropout2): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): TransformerDecoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.2, inplace=False)\n",
       "            (dropout2): Dropout(p=0.2, inplace=False)\n",
       "            (dropout3): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (encoder): Transformer(\n",
       "      (encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-3): 4 x TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.2, inplace=False)\n",
       "            (dropout2): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): TransformerDecoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.2, inplace=False)\n",
       "            (dropout2): Dropout(p=0.2, inplace=False)\n",
       "            (dropout3): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (input_embed): Embedding(4, 256)\n",
       "    (stt_encoder): ResnetStateEncoder(\n",
       "      (backbone): Joiner(\n",
       "        (0): Backbone(\n",
       "          (body): IntermediateLayerGetter(\n",
       "            (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "            (layer1): Sequential(\n",
       "              (0): BasicBlock(\n",
       "                (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn1): FrozenBatchNorm2d()\n",
       "                (relu): ReLU(inplace=True)\n",
       "                (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn2): FrozenBatchNorm2d()\n",
       "              )\n",
       "              (1): BasicBlock(\n",
       "                (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn1): FrozenBatchNorm2d()\n",
       "                (relu): ReLU(inplace=True)\n",
       "                (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn2): FrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (layer2): Sequential(\n",
       "              (0): BasicBlock(\n",
       "                (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                (bn1): FrozenBatchNorm2d()\n",
       "                (relu): ReLU(inplace=True)\n",
       "                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn2): FrozenBatchNorm2d()\n",
       "                (downsample): Sequential(\n",
       "                  (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                  (1): FrozenBatchNorm2d()\n",
       "                )\n",
       "              )\n",
       "              (1): BasicBlock(\n",
       "                (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn1): FrozenBatchNorm2d()\n",
       "                (relu): ReLU(inplace=True)\n",
       "                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn2): FrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (layer3): Sequential(\n",
       "              (0): BasicBlock(\n",
       "                (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                (bn1): FrozenBatchNorm2d()\n",
       "                (relu): ReLU(inplace=True)\n",
       "                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn2): FrozenBatchNorm2d()\n",
       "                (downsample): Sequential(\n",
       "                  (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                  (1): FrozenBatchNorm2d()\n",
       "                )\n",
       "              )\n",
       "              (1): BasicBlock(\n",
       "                (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn1): FrozenBatchNorm2d()\n",
       "                (relu): ReLU(inplace=True)\n",
       "                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn2): FrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (layer4): Sequential(\n",
       "              (0): BasicBlock(\n",
       "                (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                (bn1): FrozenBatchNorm2d()\n",
       "                (relu): ReLU(inplace=True)\n",
       "                (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn2): FrozenBatchNorm2d()\n",
       "                (downsample): Sequential(\n",
       "                  (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                  (1): FrozenBatchNorm2d()\n",
       "                )\n",
       "              )\n",
       "              (1): BasicBlock(\n",
       "                (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn1): FrozenBatchNorm2d()\n",
       "                (relu): ReLU(inplace=True)\n",
       "                (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn2): FrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PositionEmbeddingSine()\n",
       "      )\n",
       "    )\n",
       "    (image_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (image_feat_norm): LayerNorm((16, 256), eps=1e-05, elementwise_affine=True)\n",
       "    (enc_action_proj): Linear(in_features=8, out_features=256, bias=True)\n",
       "    (enc_action_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (enc_state_proj): Linear(in_features=18, out_features=256, bias=True)\n",
       "    (enc_state_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (enc_image_proj): Linear(in_features=16, out_features=1, bias=True)\n",
       "    (enc_src_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (enc_tgt_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (enc_z): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (dec_z): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (dec_input_state_proj): Linear(in_features=18, out_features=256, bias=True)\n",
       "    (dec_input_state_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (dec_input_z_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (dec_src_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (dec_tgt_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (dec_action_joint_proj): Linear(in_features=256, out_features=7, bias=True)\n",
       "    (dec_action_gripper_proj): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "      (1): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (input_embed): Embedding(3, 256)\n",
       "  (stt_encoder): ResnetStateEncoder(\n",
       "    (backbone): Joiner(\n",
       "      (0): Backbone(\n",
       "        (body): IntermediateLayerGetter(\n",
       "          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (layer1): Sequential(\n",
       "            (0): BasicBlock(\n",
       "              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "            )\n",
       "            (1): BasicBlock(\n",
       "              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (layer2): Sequential(\n",
       "            (0): BasicBlock(\n",
       "              (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): FrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): BasicBlock(\n",
       "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (layer3): Sequential(\n",
       "            (0): BasicBlock(\n",
       "              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): FrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): BasicBlock(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (layer4): Sequential(\n",
       "            (0): BasicBlock(\n",
       "              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): FrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): BasicBlock(\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): FrozenBatchNorm2d()\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): PositionEmbeddingSine()\n",
       "    )\n",
       "  )\n",
       "  (image_proj): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (image_feat_norm): LayerNorm((16, 256), eps=1e-05, elementwise_affine=True)\n",
       "  (src_state_proj): Linear(in_features=18, out_features=256, bias=True)\n",
       "  (src_state_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (src_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (tgt_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (z_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (tgt_z_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from policy import config\n",
    "from policy.dataset.ms2dataset import get_MS_loaders\n",
    "from policy.dataset.LIBEROdataset import get_LIBERO_loaders\n",
    "from policy.checkpoints import CheckpointIO\n",
    "\n",
    "model_dir = \"/home/mrl/Documents/Projects/tskill/out/PegInsertion/Plan/003\"\n",
    "\n",
    "cfg_path = os.path.join(model_dir, \"config.yaml\")\n",
    "cfg = config.load_config(cfg_path, None)\n",
    "method = cfg[\"method\"]\n",
    "if method == \"plan\":\n",
    "    cfg[\"vae_cfg\"] = config.load_config(os.path.join(cfg[\"model\"][\"vae_path\"],\"config.yaml\"))\n",
    "\n",
    "# index_path = os.path.join(model_dir, \"data_info.pickle\")\n",
    "# with open(index_path, 'rb') as f:\n",
    "#     data_info = pickle.load(f)\n",
    "\n",
    "# # Dataset\n",
    "cfg[\"data\"][\"pad\"] = False\n",
    "cfg[\"data\"][\"augment\"] = False\n",
    "cfg[\"data\"][\"full_seq\"] = False\n",
    "cfg[\"data\"][\"dataset\"] = \"/home/mrl/Documents/Projects/tskill/LIBERO/libero/datasets/libero_90/LIVING_ROOM_SCENE1_pick_up_the_tomato_sauce_and_put_it_in_the_basket_demo.hdf5\"\n",
    "\n",
    "# Load only the full episode version of the dataset\n",
    "# if \"train_ep_indices\" not in data_info.keys():\n",
    "#     train_idx, val_idx = data_info[\"train_indices\"], data_info[\"val_indices\"]\n",
    "# else:\n",
    "#     train_idx, val_idx = data_info[\"train_ep_indices\"], data_info[\"val_ep_indices\"]\n",
    "train_dataset, val_dataset = get_LIBERO_loaders(cfg, return_datasets=True, fullseq_override=True)\n",
    "\n",
    "# Model\n",
    "model = config.get_model(cfg, device=\"cpu\")\n",
    "checkpoint_io = CheckpointIO(model_dir, model=model)\n",
    "load_dict = checkpoint_io.load(\"model_best.pt\")\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 162, 9])), ('seq_pad_mask', torch.Size([1, 162])), ('skill_pad_mask', torch.Size([1, 17])), ('actions', torch.Size([1, 162, 7])), ('rgb', torch.Size([1, 162, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 196, 9])), ('seq_pad_mask', torch.Size([1, 196])), ('skill_pad_mask', torch.Size([1, 20])), ('actions', torch.Size([1, 196, 7])), ('rgb', torch.Size([1, 196, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 148, 9])), ('seq_pad_mask', torch.Size([1, 148])), ('skill_pad_mask', torch.Size([1, 15])), ('actions', torch.Size([1, 148, 7])), ('rgb', torch.Size([1, 148, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 172, 9])), ('seq_pad_mask', torch.Size([1, 172])), ('skill_pad_mask', torch.Size([1, 18])), ('actions', torch.Size([1, 172, 7])), ('rgb', torch.Size([1, 172, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 149, 9])), ('seq_pad_mask', torch.Size([1, 149])), ('skill_pad_mask', torch.Size([1, 15])), ('actions', torch.Size([1, 149, 7])), ('rgb', torch.Size([1, 149, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 149, 9])), ('seq_pad_mask', torch.Size([1, 149])), ('skill_pad_mask', torch.Size([1, 15])), ('actions', torch.Size([1, 149, 7])), ('rgb', torch.Size([1, 149, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 175, 9])), ('seq_pad_mask', torch.Size([1, 175])), ('skill_pad_mask', torch.Size([1, 18])), ('actions', torch.Size([1, 175, 7])), ('rgb', torch.Size([1, 175, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 165, 9])), ('seq_pad_mask', torch.Size([1, 165])), ('skill_pad_mask', torch.Size([1, 17])), ('actions', torch.Size([1, 165, 7])), ('rgb', torch.Size([1, 165, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 172, 9])), ('seq_pad_mask', torch.Size([1, 172])), ('skill_pad_mask', torch.Size([1, 18])), ('actions', torch.Size([1, 172, 7])), ('rgb', torch.Size([1, 172, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 156, 9])), ('seq_pad_mask', torch.Size([1, 156])), ('skill_pad_mask', torch.Size([1, 16])), ('actions', torch.Size([1, 156, 7])), ('rgb', torch.Size([1, 156, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 168, 9])), ('seq_pad_mask', torch.Size([1, 168])), ('skill_pad_mask', torch.Size([1, 17])), ('actions', torch.Size([1, 168, 7])), ('rgb', torch.Size([1, 168, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 160, 9])), ('seq_pad_mask', torch.Size([1, 160])), ('skill_pad_mask', torch.Size([1, 16])), ('actions', torch.Size([1, 160, 7])), ('rgb', torch.Size([1, 160, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 145, 9])), ('seq_pad_mask', torch.Size([1, 145])), ('skill_pad_mask', torch.Size([1, 15])), ('actions', torch.Size([1, 145, 7])), ('rgb', torch.Size([1, 145, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 152, 9])), ('seq_pad_mask', torch.Size([1, 152])), ('skill_pad_mask', torch.Size([1, 16])), ('actions', torch.Size([1, 152, 7])), ('rgb', torch.Size([1, 152, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 143, 9])), ('seq_pad_mask', torch.Size([1, 143])), ('skill_pad_mask', torch.Size([1, 15])), ('actions', torch.Size([1, 143, 7])), ('rgb', torch.Size([1, 143, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 179, 9])), ('seq_pad_mask', torch.Size([1, 179])), ('skill_pad_mask', torch.Size([1, 18])), ('actions', torch.Size([1, 179, 7])), ('rgb', torch.Size([1, 179, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 168, 9])), ('seq_pad_mask', torch.Size([1, 168])), ('skill_pad_mask', torch.Size([1, 17])), ('actions', torch.Size([1, 168, 7])), ('rgb', torch.Size([1, 168, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 158, 9])), ('seq_pad_mask', torch.Size([1, 158])), ('skill_pad_mask', torch.Size([1, 16])), ('actions', torch.Size([1, 158, 7])), ('rgb', torch.Size([1, 158, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 163, 9])), ('seq_pad_mask', torch.Size([1, 163])), ('skill_pad_mask', torch.Size([1, 17])), ('actions', torch.Size([1, 163, 7])), ('rgb', torch.Size([1, 163, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 151, 9])), ('seq_pad_mask', torch.Size([1, 151])), ('skill_pad_mask', torch.Size([1, 16])), ('actions', torch.Size([1, 151, 7])), ('rgb', torch.Size([1, 151, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 167, 9])), ('seq_pad_mask', torch.Size([1, 167])), ('skill_pad_mask', torch.Size([1, 17])), ('actions', torch.Size([1, 167, 7])), ('rgb', torch.Size([1, 167, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 171, 9])), ('seq_pad_mask', torch.Size([1, 171])), ('skill_pad_mask', torch.Size([1, 18])), ('actions', torch.Size([1, 171, 7])), ('rgb', torch.Size([1, 171, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 182, 9])), ('seq_pad_mask', torch.Size([1, 182])), ('skill_pad_mask', torch.Size([1, 19])), ('actions', torch.Size([1, 182, 7])), ('rgb', torch.Size([1, 182, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 215, 9])), ('seq_pad_mask', torch.Size([1, 215])), ('skill_pad_mask', torch.Size([1, 22])), ('actions', torch.Size([1, 215, 7])), ('rgb', torch.Size([1, 215, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 148, 9])), ('seq_pad_mask', torch.Size([1, 148])), ('skill_pad_mask', torch.Size([1, 15])), ('actions', torch.Size([1, 148, 7])), ('rgb', torch.Size([1, 148, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 184, 9])), ('seq_pad_mask', torch.Size([1, 184])), ('skill_pad_mask', torch.Size([1, 19])), ('actions', torch.Size([1, 184, 7])), ('rgb', torch.Size([1, 184, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 156, 9])), ('seq_pad_mask', torch.Size([1, 156])), ('skill_pad_mask', torch.Size([1, 16])), ('actions', torch.Size([1, 156, 7])), ('rgb', torch.Size([1, 156, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 158, 9])), ('seq_pad_mask', torch.Size([1, 158])), ('skill_pad_mask', torch.Size([1, 16])), ('actions', torch.Size([1, 158, 7])), ('rgb', torch.Size([1, 158, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 135, 9])), ('seq_pad_mask', torch.Size([1, 135])), ('skill_pad_mask', torch.Size([1, 14])), ('actions', torch.Size([1, 135, 7])), ('rgb', torch.Size([1, 135, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 165, 9])), ('seq_pad_mask', torch.Size([1, 165])), ('skill_pad_mask', torch.Size([1, 17])), ('actions', torch.Size([1, 165, 7])), ('rgb', torch.Size([1, 165, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 188, 9])), ('seq_pad_mask', torch.Size([1, 188])), ('skill_pad_mask', torch.Size([1, 19])), ('actions', torch.Size([1, 188, 7])), ('rgb', torch.Size([1, 188, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 180, 9])), ('seq_pad_mask', torch.Size([1, 180])), ('skill_pad_mask', torch.Size([1, 18])), ('actions', torch.Size([1, 180, 7])), ('rgb', torch.Size([1, 180, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 162, 9])), ('seq_pad_mask', torch.Size([1, 162])), ('skill_pad_mask', torch.Size([1, 17])), ('actions', torch.Size([1, 162, 7])), ('rgb', torch.Size([1, 162, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 133, 9])), ('seq_pad_mask', torch.Size([1, 133])), ('skill_pad_mask', torch.Size([1, 14])), ('actions', torch.Size([1, 133, 7])), ('rgb', torch.Size([1, 133, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 161, 9])), ('seq_pad_mask', torch.Size([1, 161])), ('skill_pad_mask', torch.Size([1, 17])), ('actions', torch.Size([1, 161, 7])), ('rgb', torch.Size([1, 161, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 184, 9])), ('seq_pad_mask', torch.Size([1, 184])), ('skill_pad_mask', torch.Size([1, 19])), ('actions', torch.Size([1, 184, 7])), ('rgb', torch.Size([1, 184, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 143, 9])), ('seq_pad_mask', torch.Size([1, 143])), ('skill_pad_mask', torch.Size([1, 15])), ('actions', torch.Size([1, 143, 7])), ('rgb', torch.Size([1, 143, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 151, 9])), ('seq_pad_mask', torch.Size([1, 151])), ('skill_pad_mask', torch.Size([1, 16])), ('actions', torch.Size([1, 151, 7])), ('rgb', torch.Size([1, 151, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 137, 9])), ('seq_pad_mask', torch.Size([1, 137])), ('skill_pad_mask', torch.Size([1, 14])), ('actions', torch.Size([1, 137, 7])), ('rgb', torch.Size([1, 137, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 156, 9])), ('seq_pad_mask', torch.Size([1, 156])), ('skill_pad_mask', torch.Size([1, 16])), ('actions', torch.Size([1, 156, 7])), ('rgb', torch.Size([1, 156, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 147, 9])), ('seq_pad_mask', torch.Size([1, 147])), ('skill_pad_mask', torch.Size([1, 15])), ('actions', torch.Size([1, 147, 7])), ('rgb', torch.Size([1, 147, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 192, 9])), ('seq_pad_mask', torch.Size([1, 192])), ('skill_pad_mask', torch.Size([1, 20])), ('actions', torch.Size([1, 192, 7])), ('rgb', torch.Size([1, 192, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 168, 9])), ('seq_pad_mask', torch.Size([1, 168])), ('skill_pad_mask', torch.Size([1, 17])), ('actions', torch.Size([1, 168, 7])), ('rgb', torch.Size([1, 168, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 172, 9])), ('seq_pad_mask', torch.Size([1, 172])), ('skill_pad_mask', torch.Size([1, 18])), ('actions', torch.Size([1, 172, 7])), ('rgb', torch.Size([1, 172, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n",
      "[('goal', torch.Size([1, 1, 2, 3, 128, 128])), ('state', torch.Size([1, 134, 9])), ('seq_pad_mask', torch.Size([1, 134])), ('skill_pad_mask', torch.Size([1, 14])), ('actions', torch.Size([1, 134, 7])), ('rgb', torch.Size([1, 134, 2, 3, 128, 128])), ('dec_src_mask', torch.Size([1, 331, 331])), ('dec_mem_mask', torch.Size([1, 10, 331])), ('dec_tgt_mask', torch.Size([1, 10, 10])), ('enc_src_mask', torch.Size([1, 215, 215])), ('enc_mem_mask', torch.Size([1, 21, 215])), ('enc_tgt_mask', torch.Size([1, 21, 21])), ('plan_tgt_mask', torch.Size([1, 21, 21])), ('plan_src_mask', torch.Size([1, 725, 725])), ('plan_mem_mask', torch.Size([1, 21, 725]))]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "for i in range(len(train_dataset)):\n",
    "    print([(k,v.shape) for k,v in train_dataset[i].items()])\n",
    "\n",
    "plt.imshow(train_dataset[5][\"rgb\"][0,5,0,:,:,:].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Training Dataset\n",
      "torch.Size([2, 152])\n",
      "==>> batch_mask: tensor([False,  True])\n",
      "==>> out: {'a_hat': tensor([[[-0.5221,  0.1630, -0.9107,  ...,  0.1944,  0.3796,  0.9750],\n",
      "         [-0.5171, -0.0427, -0.8406,  ...,  0.3647,  0.2453,  0.9784],\n",
      "         [-0.4795,  0.1366, -0.9926,  ...,  0.4508,  0.5319,  0.9671],\n",
      "         ...,\n",
      "         [ 0.7082,  0.6392,  0.4021,  ..., -0.9295,  0.6603, -0.9849],\n",
      "         [ 0.5679,  0.6016,  0.6472,  ..., -0.6540,  0.4302, -0.9721],\n",
      "         [ 0.6214,  0.4352,  0.6776,  ..., -0.6956,  0.4620, -0.9791]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<PermuteBackward0>), 'mu': tensor([[[ 0.7644,  0.9377, -0.7649,  ...,  1.4079,  3.0364,  1.5941],\n",
      "         [-0.0932,  1.1420, -0.5974,  ...,  2.1631,  2.9671,  0.5113],\n",
      "         [ 0.3839,  1.1029, -0.6836,  ...,  2.0320,  1.7729,  0.8207],\n",
      "         ...,\n",
      "         [ 0.2583,  1.4258, -0.8658,  ...,  2.9261,  3.2378,  1.5384],\n",
      "         [-0.0743,  0.7135,  0.2276,  ...,  1.9054,  2.9838,  1.7257],\n",
      "         [ 0.2852,  0.8849, -0.3207,  ...,  2.1214,  3.3927,  0.7846]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<PermuteBackward0>), 'logvar': tensor([[[-0.7996, -0.1817, -0.2805,  ..., -0.9518,  0.8677, -2.3791],\n",
      "         [-1.2163,  0.1138,  0.2337,  ..., -0.9143,  0.4249, -1.7778],\n",
      "         [-0.1874,  0.6840, -0.4452,  ..., -1.6086,  0.6797, -2.1222],\n",
      "         ...,\n",
      "         [-0.7136, -0.7445, -0.0323,  ..., -1.2600, -0.3201, -2.3524],\n",
      "         [-1.6034,  0.0261, -0.2896,  ..., -0.6019,  0.4469, -2.2104],\n",
      "         [-1.1884,  0.2875, -0.0404,  ..., -0.5171, -0.1924, -1.7695]],\n",
      "\n",
      "        [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         ...,\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]]],\n",
      "       grad_fn=<PermuteBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "train = True\n",
    "if not train:\n",
    "    dataset = val_dataset\n",
    "    print(\"Using Validation Dataset\")\n",
    "    idxs = val_idx\n",
    "else:\n",
    "    dataset = train_dataset\n",
    "    print(\"Using Training Dataset\")\n",
    "    idxs = train_idx\n",
    "\n",
    "i = 1\n",
    "data = dataset[i]\n",
    "# with torch.no_grad():\n",
    "data = {k: torch.vstack((v,v)) for k,v in data.items()}\n",
    "data[\"seq_pad_mask\"][0,...] = torch.ones_like(data[\"seq_pad_mask\"][0,...])\n",
    "data[\"skill_pad_mask\"][0,...] = torch.ones_like(data[\"skill_pad_mask\"][0,...])\n",
    "print(data[\"seq_pad_mask\"].shape)\n",
    "model.zero_grad()\n",
    "out = model(data)\n",
    "print(f\"==>> out: {out}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tskill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
